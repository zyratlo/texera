# See PR https://github.com/Texera/texera/pull/3326 for configuration guidelines.
constants {
    logging-queue-size-interval = 30000
    logging-queue-size-interval = ${?CONSTANTS_LOGGING_QUEUE_SIZE_INTERVAL}

    num-worker-per-operator = 2
    num-worker-per-operator = ${?CONSTANTS_NUM_WORKER_PER_OPERATOR}

    max-resolution-rows = 2000
    max-resolution-rows = ${?CONSTANTS_MAX_RESOLUTION_ROWS}

    max-resolution-columns = 2000
    max-resolution-columns = ${?CONSTANTS_MAX_RESOLUTION_COLUMNS}

    status-update-interval = 500
    status-update-interval = ${?CONSTANTS_STATUS_UPDATE_INTERVAL}
}

flow-control {
    max-credit-allowed-in-bytes-per-channel = 1600000000  # -1 to disable flow control
    max-credit-allowed-in-bytes-per-channel = ${?FLOW_CONTROL_MAX_CREDIT_ALLOWED_IN_BYTES_PER_CHANNEL}

    credit-poll-interval-in-ms = 200
    credit-poll-interval-in-ms = ${?FLOW_CONTROL_CREDIT_POLL_INTERVAL_IN_MS}
}

network-buffering {
    default-data-transfer-batch-size = 400
    default-data-transfer-batch-size = ${?NETWORK_BUFFERING_DEFAULT_DATA_TRANSFER_BATCH_SIZE}

    enable-adaptive-buffering = true
    enable-adaptive-buffering = ${?NETWORK_BUFFERING_ENABLE_ADAPTIVE_BUFFERING}

    adaptive-buffering-timeout-ms = 500
    adaptive-buffering-timeout-ms = ${?NETWORK_BUFFERING_ADAPTIVE_BUFFERING_TIMEOUT_MS}
}

reconfiguration {
    enable-transactional-reconfiguration = false
    enable-transactional-reconfiguration = ${?RECONFIGURATION_ENABLE_TRANSACTIONAL_RECONFIGURATION}
}

cache {
    # [false, true]
    enabled = true
    enabled = ${?CACHE_ENABLED}
}

user-sys {
    enabled = false
    enabled = ${?USER_SYS_ENABLED}

    google {
        clientId = ""
        clientId = ${?USER_SYS_GOOGLE_CLIENT_ID}

        smtp {
            gmail = ""
            gmail = ${?USER_SYS_GOOGLE_SMTP_GMAIL}

            password = ""
            password = ${?USER_SYS_GOOGLE_SMTP_PASSWORD}
        }
    }

    version-time-limit-in-minutes = 60
    version-time-limit-in-minutes = ${?USER_SYS_VERSION_TIME_LIMIT_IN_MINUTES}
}

result-cleanup {
    ttl-in-seconds = 86400 # time to live for a collection is 2 days
    ttl-in-seconds = ${?RESULT_CLEANUP_TTL_IN_SECONDS}

    collection-check-interval-in-seconds = 86400 # 2 days
    collection-check-interval-in-seconds = ${?RESULT_CLEANUP_COLLECTION_CHECK_INTERVAL_IN_SECONDS}
}

web-server {
    workflow-state-cleanup-in-seconds = 30
    workflow-state-cleanup-in-seconds = ${?WEB_SERVER_WORKFLOW_STATE_CLEANUP_IN_SECONDS}

    python-console-buffer-size = 100
    python-console-buffer-size = ${?WEB_SERVER_PYTHON_CONSOLE_BUFFER_SIZE}

    console-message-max-display-length = 100
    console-message-max-display-length = ${?WEB_SERVER_CONSOLE_MESSAGE_MAX_DISPLAY_LENGTH}

    workflow-result-pulling-in-seconds = 3
    workflow-result-pulling-in-seconds = ${?WEB_SERVER_WORKFLOW_RESULT_PULLING_IN_SECONDS}

    clean-all-execution-results-on-server-start = false
    clean-all-execution-results-on-server-start = ${?WEB_SERVER_CLEAN_ALL_EXECUTION_RESULTS_ON_SERVER_START}
}

fault-tolerance {
    # URI for storage, empty to disable logging.
    # Use absolute path only. for local file system, $AMBER_FOLDER will be interpolated to Amber folder path.
    # e.g. use "file://$AMBER_FOLDER/../log/recovery-logs/" for local logging.
    log-storage-uri = ""
    log-storage-uri = ${?FAULT_TOLERANCE_LOG_STORAGE_URI}

    log-flush-interval-ms = 0 # immediately flush
    log-flush-interval-ms = ${?FAULT_TOLERANCE_LOG_FLUSH_INTERVAL_MS}

    log-record-max-size-in-byte = 67108864 # 64MB
    log-record-max-size-in-byte = ${?FAULT_TOLERANCE_LOG_RECORD_MAX_SIZE_IN_BYTE}

    # limit for resend buffer length, if the resend buffer
    # getting too large, the workflow aborts during recovery to avoid OOM.
    # TODO: Remove this after introducing checkpoints.
    max-supported-resend-queue-length = 10000
    max-supported-resend-queue-length = ${?FAULT_TOLERANCE_MAX_SUPPORTED_RESEND_QUEUE_LENGTH}

    delay-before-recovery = 3000
    delay-before-recovery = ${?FAULT_TOLERANCE_DELAY_BEFORE_RECOVERY}

    hdfs-storage {
        address = "0.0.0.0:9870"
        address = ${?FAULT_TOLERANCE_HDFS_STORAGE_ADDRESS}
    }
}

schedule-generator {
    enable-cost-based-schedule-generator = false
    enable-cost-based-schedule-generator = ${?SCHEDULE_GENERATOR_ENABLE_COST_BASED_SCHEDULE_GENERATOR}

    use-global-search = false
    use-global-search = ${?SCHEDULE_GENERATOR_USE_GLOBAL_SEARCH}

    use-top-down-search = false
    use-top-down-search = ${?SCHEDULE_GENERATOR_USE_TOP_DOWN_SEARCH}

    search-timeout-milliseconds = 1000
    search-timeout-milliseconds = ${?SCHEDULE_GENERATOR_SEARCH_TIMEOUT_MILLISECONDS}
}

ai-assistant-server {
    assistant = "none"
    assistant = ${?AI_ASSISTANT_SERVER_ASSISTANT}

    # Put your Ai Service authentication key here
    ai-service-key = ""
    ai-service-key = ${?AI_ASSISTANT_SERVER_AI_SERVICE_KEY}

    # Put your Ai service url here (If you are using OpenAI, then the url should be "https://api.openai.com/v1")
    ai-service-url = ""
    ai-service-url = ${?AI_ASSISTANT_SERVER_AI_SERVICE_URL}
}
